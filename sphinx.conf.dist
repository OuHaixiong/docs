# 几个概念：
# source：数据源，数据是从什么地方来的
# index：索引，当有数据源之后，从数据源处构建索引。索引：实际上就是相当于一个字典检索。有了整本字典内容以后，才会有字典检索。
# searchd：提供搜索查询服务。它一般是以deamon的形式运行在后台的。
# indexer：构建索引的服务。当要重新构建索引的时候，就是调用indexer这个命令。
# attr：属性，属性是存在索引中的，它不进行全文索引，但是可以用于过滤和排序。


#
# Sphinx configuration file sample
#
# WARNING! While this sample file mentions all available options,
# it contains (very) short helper descriptions only. Please refer to
# doc/sphinx.html for details.
#

#############################################################################
## data source definition
#############################################################################

source src1 # 数据源名称
{
	# data source type. mandatory, no default value
	# known types are mysql, pgsql, mssql, xmlpipe, xmlpipe2, odbc
	type			= mysql # 数据源类型

	#####################################################################
	## SQL settings (for 'mysql' and 'pgsql' types)
	#####################################################################

	# some straightforward parameters for SQL source types
	sql_host		= localhost # 要连接的SQL服务器主机地址。仅对 SQL数据源（mysql和pgsql）有效
	sql_user		= test # 连接到sql_host时使用的SQL用户名
	sql_pass		=      # 连接到sql_host时使用的SQL用户密码
	sql_db			= test # 连接到SQL数据源之后使用的SQL数据库
	sql_port		= 3306	# optional, default is 3306。要连接的SQL服务器的端口

	# UNIX socket name
	# optional, default is empty (reuse client library defaults)
	# usually '/var/lib/mysql/mysql.sock' on Linux
	# usually '/tmp/mysql.sock' on FreeBSD
	#
	# sql_sock		= /tmp/mysql.sock # 连接到本地SQL服务器时使用的UNIX socket名称。


	# MySQL specific client connection flags 仅适用于mysql数据源
	# optional, default is 0
	# indexer和mysql之间的交互，需要考虑到效率和安全性。
	# 比如考虑到效率，他们两者之间的交互需要使用压缩协议；考虑到安全，他们两者之间的传输需要使用ssl
	# 那么这个参数就代表这个意思，0/32/2048/32768  无/使用压缩协议/握手后切换到ssl/Mysql 4.1版本身份认证。
	# mysql_connect_flags	= 32 # enable compression

	# MySQL specific SSL certificate settings MySQL的SSL连接（仅适用于mysql数据源）
	# optional, defaults are empty
	# 当mysql_connect_flags设置为2048（ssl）的时候，下面几个就代表ssl连接所需要使用的几个参数。
	# mysql_ssl_cert		= /etc/ssl/client-cert.pem
	# mysql_ssl_key		= /etc/ssl/client-key.pem
	# mysql_ssl_ca		= /etc/ssl/cacert.pem

	# MS SQL specific Windows authentication mode flag
	# MUST be in sync with charset_type index-level setting
	# optional, default is 0
	# mssql特有，是否使用windows登陆
	# mssql_winauth		= 1 # use currently logged on user credentials


	# ODBC specific DSN (data source name)  仅适用于odbc数据源
	# mandatory for odbc source type, no default value
	# odbc的dsn串
	# odbc_dsn		= DBQ=C:\data;DefaultDir=C:\data;Driver={Microsoft Text Driver (*.txt; *.csv)};
	# sql_query		= SELECT id, data FROM documents.csv


	# ODBC and MS SQL specific, per-column buffer sizes  
	# optional, default is auto-detect
	# sql某一列的缓冲大小，一般是针对字符串来说的。
    # 为什么要有这么一种缓冲呢？
    # 有的字符串，虽然长度很长，但是实际上并没有使用那么长的字符，所以在Sphinx并不会收录所有的字符，而是给每个属性一个缓存作为长度限制。
    # 默认情况下非字符类型的属性是1KB，字符类型的属性是1MB。
    # 而如果想要配置这个buffer的话，就可以在这里进行配置了。
	# sql_column_buffers	= content=12M, comments=1M


	# pre-query, executed before the main fetch query
	# multi-value, optional, default is empty list of queries
	# indexer的sql执行前需要执行的操作。（如果需要执行多条sql语句，就写多条sql_query_pre）
	# sql_query_pre		= SET NAMES utf8 # mysql特有sql语句
	# sql_query_pre		= SET SESSION query_cache_type=OFF


	# main document fetch query
	# mandatory, integer document ID field MUST be the first selected column
	# indexer的sql执行语句。文档ID必须是第一列，且必须是大于0的整数
	# 全文检索要显示的内容，在这里尽可能不使用where或group by，将where与groupby的内容交给sphinx，由sphinx进行条件过滤与groupby效率会更高
	sql_query		= \
		SELECT id, group_id, UNIX_TIMESTAMP(date_added) AS date_added, title, content \
		FROM documents


	# joined/payload field fetch query
	# joined fields let you avoid (slow) JOIN and GROUP_CONCAT
	# payload fields let you attach custom per-keyword values (eg. for ranking)
	#
	# syntax is FIELD-NAME 'from'  ( 'query' | 'payload-query' ); QUERY
	# joined field QUERY should return 2 columns (docid, text)
	# payload field QUERY should return 3 columns (docid, keyword, weight)
	#
	# REQUIRES that query results are in ascending document ID order!
	# multi-value, optional, default is empty list of queries
	# 有的时候有多个表，我们想要查询的字段在其他表中。这个时候就需要对sql_query进行join操作。
    # 而这个join操作可能非常慢，导致建立索引的时候特别慢，那么这个时候，就可以考虑在sphinx端进行join操作了。
    # sql_joined_field是增加一个字段，这个字段是从其他表查询中查询出来的。
    # 这里分号后面的查询语句是有要求的，如果是query，则返回id和查询字段，如果是payload-query，则返回id，查询字段和权重。
    # 并且这里的后一个查询需要按照id进行升序排列。
	# sql_joined_field	= tags from query; SELECT docid, CONCAT('tag',tagid) FROM tags ORDER BY docid ASC
	# sql_joined_field	= wtags from payload-query; SELECT docid, tag, tagweight FROM tags ORDER BY docid ASC


	# file based field declaration
	#
	# content of this field is treated as a file name
	# and the file gets loaded and indexed in place of a field
	#
	# max file size is limited by max_file_field_buffer indexer setting
	# file IO errors are non-fatal and get reported as warnings
	# 外部文件字段，意思就是一个表中，有一个字段存的是外部文件地址，但是实际的字段内容在文件中。比如这个字段叫做content_file_path。
    # 当indexer建立索引的时候，查到这个字段，就读取这个文件地址，然后加载，并进行分词和索引建立等操作。
	# sql_file_field		= content_file_path


	# range query setup, query that must return min and max ID values
	# optional, default is empty
	#
	# sql_query will need to reference $start and $end boundaries
	# if using ranged query:
	#
	# sql_query		= \
	#	SELECT doc.id, doc.id AS group, doc.title, doc.data \
	#	FROM documents doc \
	#	WHERE id>=$start AND id<=$end
	# 当数据源数据太大的时候，一个sql语句查询下来往往很有可能锁表等操作。
    # 那么我么就可以使用多次查询，那么这个多次查询就需要有个范围和步长，sql_query_range和sql_range_step就是做这个使用的。
    # 获取最大和最小的id，然后根据步长来获取数据。比如下面的例子，如果有4500条数据，这个表建立索引的时候就会进行5次sql查询。 
    # 而5次sql查询每次的间隔时间是使用sql_ranged_throttle来进行设置的。单位是毫秒。
	# sql_query_range		= SELECT MIN(id),MAX(id) FROM documents
	
	# range query step
	# optional, default is 1024
	# sql_range_step		= 1000
	
    # ranged query throttling, in milliseconds
    # optional, default is 0 which means no delay
    # enforces given delay before each query step
    sql_ranged_throttle = 0
    
    # 下面都是些不同属性的数据了
    # 先要了解属性的概念：属性是存在索引中的，它不进行全文索引，但是可以用于过滤和排序。
    
	# unsigned integer attribute declaration
	# multi-value (an arbitrary number of attributes is allowed), optional
	# optional bit size can be specified, default is 32
	#
	# sql_attr_uint		= author_id
	# sql_attr_uint		= forum_id:9 # 9 bits for forum_id
	sql_attr_uint		= group_id # uint无符号整型属性

	# boolean attribute declaration
	# multi-value (an arbitrary number of attributes is allowed), optional
	# equivalent to sql_attr_uint with 1-bit size
	#
	# sql_attr_bool		= is_deleted # bool属性


	# bigint attribute declaration
	# multi-value (an arbitrary number of attributes is allowed), optional
	# declares a signed (unlike uint!) 64-bit attribute
	#
	# sql_attr_bigint		= my_bigint_id # 长整型属性


	# UNIX timestamp attribute declaration
	# multi-value (an arbitrary number of attributes is allowed), optional
	# similar to integer, but can also be used in date functions
	#
	# sql_attr_timestamp	= posted_ts
	# sql_attr_timestamp	= last_edited_ts
	sql_attr_timestamp	= date_added # 时间戳属性，经常被用于做排序

    ## 字符串排序属性。一般我们按照字符串排序的话，我们会将这个字符串存下来进入到索引中，然后在查询的时候比较索引中得字符大小进行排序。
    ## 但是这个时候索引就会很大，于是我们就想到了一个方法，我们在建立索引的时候，先将字符串值从数据库中取出，暂存，排序。
    ## 然后给排序后的数组分配一个序号，然后在建立索引的时候，就将这个序号存入到索引中去。这样在查询的时候也就能完成字符串排序的操作。
    ## 这，就是这个字段的意义。（貌似这个配置在最新版本中已废除）
    # sql_attr_str2ordinal  = author_name

	# floating point attribute declaration
	# multi-value (an arbitrary number of attributes is allowed), optional
	# values are stored in single precision, 32-bit IEEE 754 format
	#
	# sql_attr_float		= lat_radians # 浮点数属性，经常在查询地理经纬度的时候会用到。
	# sql_attr_float		= long_radians


	# multi-valued attribute (MVA) attribute declaration
	# multi-value (an arbitrary number of attributes is allowed), optional
	# MVA values are variable length lists of unsigned 32-bit integers
	#
	# syntax is ATTR-TYPE ATTR-NAME 'from' SOURCE-TYPE [;QUERY] [;RANGE-QUERY]
	# ATTR-TYPE is 'uint' or 'timestamp'
	# SOURCE-TYPE is 'field', 'query', or 'ranged-query'
	# QUERY is SQL query used to fetch all ( docid, attrvalue ) pairs
	# RANGE-QUERY is SQL query used to fetch min and max ID values, similar to 'sql_query_range'
	# 多值属性（MVA）
    # 试想一下，有一个文章系统，每篇文章都有多个标签，这个文章就叫做多值属性。
    # 我要对某个标签进行查询过滤，那么在建立查询的时候就应该把这个标签的值放入到索引中。
    # 这个字段，sql_attr_multi就是用来做这个事情的。
	# sql_attr_multi		= uint tag from query; SELECT docid, tagid FROM tags
	# sql_attr_multi		= uint tag from ranged-query; \
	#	SELECT docid, tagid FROM tags WHERE id>=$start AND id<=$end; \
	#	SELECT MIN(docid), MAX(docid) FROM tags


	# string attribute declaration
	# multi-value (an arbitrary number of these is allowed), optional
	# lets you store and retrieve strings
	#
	# sql_attr_string		= stitle # 字符串属性。


	# JSON attribute declaration
	# multi-value (an arbitrary number of these is allowed), optional
	# lets you store a JSON document as an (in-memory) attribute for later use
	#
	# sql_attr_json		= properties


	# combined field plus attribute declaration (from a single column)
	# stores column as an attribute, but also indexes it as a full-text field
	#
	# sql_field_string	= author # 字符串字段，可全文搜索，可返回原始文本信息。

	
	# post-query, executed on sql_query completion
	# optional, default is empty
	# 取后查询，在sql_query执行后立即操作。
    # 它和sql_query_post_index的区别就是执行时间不同
    # sql_query_post是在sql_query执行后执行，而sql_query_post_index是在索引建立完成后才执行。
    # 所以如果要记录最后索引执行时间，那么应该在sql_query_post_index中执行。
	# sql_query_post		=

	
	# post-index-query, executed on successful indexing completion
	# optional, default is empty
	# $maxid expands to max document ID actually fetched from DB
	# 索引建立完成后执行
	# sql_query_post_index	= REPLACE INTO counters ( id, val ) \
	#	VALUES ( 'max_indexed_id', $maxid )

	# kill-list query, fetches the document IDs for kill-list
	# k-list will suppress matches from preceding indexes in the same query
	# optional, default is empty
	# 比如有两个索引，一个索引比较旧，一个索引比较新，那么旧索引中就会有数据是旧的。
    # 当我要对两个索引进行搜索的时候，哪些数据要按照新的索引来进行查询呢。
    # 这个时候就使用到了这个字段了
	# sql_query_killlist	= SELECT id FROM documents WHERE edited>=@last_reindex

    ## 命令行获取信息查询。
    ## 什么意思呢？
    ## 我们进行索引一般只会返回主键id，而不会返回表中的所有字段。
    ## 但是在调试的时候，我们一般需要返回表中的字段，那这个时候，就需要使用sql_query_info。
    ## 同时这个字段只在控制台有效，在api中是无效的。（貌似在新版中此配置作废）
    sql_query_info      = SELECT * FROM documents WHERE id=$id

	# columns to unpack on indexer side when indexing
	# multi-value, optional, default is empty list
	# 下面几个压缩解压的配置都是为了一个目的：让索引重建的时候不要影响数据库的性能表现。
	# unpack_zlib		= zlib_column # SQL数据源解压字段设置
	# unpack_mysqlcompress	= compressed_column # MySQL数据源解压字段设置
	# unpack_mysqlcompress	= compressed_column_2


	# maximum unpacked length allowed in MySQL COMPRESS() unpacker
	# optional, default is 16M
	# MySQL数据源解压缓冲区设置
	# unpack_mysqlcompress_maxsize	= 16M


	# hook command to run when SQL connection succeeds
	# optional, default value is empty (do nothing)
	#
	# hook_connect			= bash sql_connect.sh


	# hook command to run after (any) SQL range query
	# it may print out "minid maxid" (w/o quotes) to override the range
	# optional, default value is empty (do nothing)
	#
	# hook_query_range		= bash sql_query_range.sh


	# hook command to run on successful indexing completion
	# $maxid expands to max document ID actually fetched from DB
	# optional, default value is empty (do nothing)
	#
	# hook_post_index		= bash sql_post_index.sh $maxid

	#####################################################################
	## xmlpipe2 settings
	#####################################################################

	# type			= xmlpipe # xmlpipe的数据源就是一个xml文档

	# shell command to invoke xmlpipe stream producer
	# mandatory
	# 读取数据源的命令
	# xmlpipe_command		= cat /var/test.xml

	# xmlpipe2 field declaration
	# multi-value, optional, default is empty
	# 字段
	# xmlpipe_field		= subject
	# xmlpipe_field		= content


	# xmlpipe2 attribute declaration
	# multi-value, optional, default is empty
	# all xmlpipe_attr_XXX options are fully similar to sql_attr_XXX
	# examples:
	# 属性
	# xmlpipe_attr_timestamp	= published
	# xmlpipe_attr_uint	= author_id
	# xmlpipe_attr_bool	= is_enabled
	# xmlpipe_attr_float	= latitude
	# xmlpipe_attr_bigint	= guid
	# xmlpipe_attr_multi	= tags
	# xmlpipe_attr_multi_64	= tags64
	# xmlpipe_attr_string	= title
	# xmlpipe_attr_json	= extra_data
	# xmlpipe_field_string	= content


	# perform UTF-8 validation, and filter out incorrect codes
	# avoids XML parser choking on non-UTF-8 documents
	# optional, default is 0
	# UTF-8修复设置
    # 只适用xmlpipe2数据源，数据源中有可能有非utf-8的字符，这个时候解析就有可能出现问题
    # 如果设置了这个字段，非utf-8序列就会全部被替换为空格。
	# xmlpipe_fixup_utf8	= 1
}


# inherited source example
# sphinx的source是有继承这么一种属性的，意思就是除了父source之外，这个source还有这个特性
# all the parameters are copied from the parent source,
# and may then be overridden in this source definition
source src1throttled : src1 # 继承
{
	sql_ranged_throttle	= 100 # 多次执行sql时，每次执行的间隔时间，单位：毫秒
}

#############################################################################
## index definition
#############################################################################

# local index example
#
# this is an index which is stored locally in the filesystem
#
# all indexing-time options (such as morphology and charsets)
# are configured per local index
index test1 # 索引
{
	# index type
	# optional, default is 'plain'
	# known values are 'plain', 'distributed', and 'rt' (see samples below)
	# 索引类型，包括有plain，distributed和rt。分别是普通索引/分布式索引/实时索引。默认是plain。
	# type			= plain

	# document source(s) to index
	# multi-value, mandatory
	# document IDs must be globally unique across all sources
	source			= src1 # 索引数据源

	# index files path and file name, without extension
	# mandatory, path must be writable, extensions will be auto-appended
	path			= /var/data/test1 # 索引文件存放路径。如：path = /data/db/sphinx/product 会再product文件夹中生产多个索引文件

	# document attribute values (docinfo) storage mode
	# optional, default is 'extern'
	# known values are 'none', 'extern' and 'inline'
	# 文档信息的存储模式，包括有none,extern,inline。默认是extern。
    # docinfo指的就是数据的所有属性（field）构成的一个集合。
    # 首先文档id是存储在一个文件中的（spa）
    # 当使用inline的时候，文档的属性和文件的id都是存放在spa中的，所以进行查询过滤的时候，不需要进行额外操作。
    # 当使用extern的时候，文档的属性是存放在另外一个文件（spd）中的，但是当启动searchd的时候，会把这个文件加载到内存中。
    # extern就意味着每次做查询过滤的时候，除了查找文档id之外，还需要去内存中根据属性进行过滤。
    # 但是即使这样，extern由于文件太小，效率也不低。所以不是有特殊要求，一般都是使用extern
	# docinfo			= extern   # 这个配置在最新版中也已废除

	# dictionary type, 'crc' or 'keywords'
	# crc is faster to index when no substring/wildcards searches are needed
	# crc with substrings might be faster to search but is much slower to index
	# (because all substrings are pre-extracted as individual keywords)
	# keywords is much faster to index with substrings, and index is much (3-10x) smaller
	# keywords supports wildcards, crc does not, and never will
	# optional, default is 'keywords'
	dict			= keywords

	# memory locking for cached data (.spa and .spi), to prevent swapping
	# optional, default is 0 (do not mlock)
	# requires searchd to be run from root
	# 缓冲内存锁定。
    # searchd会讲spa和spi预读取到内存中。但是如果这部分内存数据长时间没有访问，则它会被交换到磁盘上。
    # 设置了mlock就不会出现这个问题，这部分数据会一直存放在内存中的。
	mlock			= 0

	# a list of morphology preprocessors to apply
	# optional, default is empty
	#
	# builtin preprocessors are 'none', 'stem_en', 'stem_ru', 'stem_enru',
	# 'soundex', and 'metaphone'; additional preprocessors available from
	# libstemmer are 'libstemmer_XXX', where XXX is algorithm code
	# (see libstemmer_c/libstemmer/modules.txt)
	#
	# morphology		= stem_en, stem_ru, soundex
	# morphology		= libstemmer_german
	# morphology		= libstemmer_sv
	# 词形处理器
    # 词形处理是什么意思呢？比如在英语中，dogs是dog的复数，所以dog是dogs的词干，这两个实际上是同一个词。
    # 所以英语的词形处理器会讲dogs当做dog来进行处理。对中文无效
	morphology		= none

	# minimum word length at which to enable stemming
	# optional, default is 1 (stem everything)
	# 词形处理有的时候会有问题，比如将gps处理成gp，这个设置可以允许根据词的长度来决定是否要使用词形处理器
	# min_stemming_len	= 1


	# stopword files list (space separated)
	# optional, default is empty
	# contents are plain text, charset_table and stemming are both applied
	# 停止词，停止词是不被索引的词。
	# stopwords		= /var/data/stopwords.txt


	# wordforms file, in "mapfrom > mapto" plain text format
	# optional, default is empty
	# 自定义词形字典
	# wordforms		= /var/data/wordforms.txt


	# tokenizing exceptions file
	# optional, default is empty
	#
	# plain text, case sensitive, space insensitive in map-from part
	# one "Map Several Words => ToASingleOne" entry per line
	# 词汇特殊处理。
    # 有的一些特殊词我们希望把它当成另外一个词来处理。比如，c++ => cplusplus来处理
	# exceptions		= /var/data/exceptions.txt


	# embedded file size limit
	# optional, default is 16K
	#
	# exceptions, wordforms, and stopwords files smaller than this limit
	# are stored in the index; otherwise, their paths and sizes are stored
	#
	# embedded_limit		= 16K

	# minimum indexed word length
	# default is 1 (index everything)
	# 最小索引词长度，小于这个长度的词不会被索引
	min_word_len		= 1

    # 字符集编码类型，可以为sbcs,utf-8。此配置在最新版里已废除
    # charset_type        = sbcs
    
    # 字符表和大小写转换规则。
    # 'sbcs' default value is
    # charset_table     = 0..9, A..Z->a..z, _, a..z, U+A8->U+B8, U+B8, U+C0..U+DF->U+E0..U+FF, U+E0..U+FF
    #
    # 'utf-8' default value is # 已验证，搜索中文此配置必填
    charset_table     = 0..9, A..Z->a..z, _, a..z, U+410..U+42F->U+430..U+44F, U+430..U+44F
    
	# ignored characters list
	# optional, default value is empty
	# 忽略字符表。在忽略字符表中的前后词会被连起来当做一个单独关键词处理。
	# ignore_chars		= U+00AD
	
    # 是否启用通配符，默认为0，不启用（貌似可能在新sphinx中已废除）
    # enable_star       = 1

    # min_prefix_len,min_infix_len,prefix_fields,infix_fields都是在enable_star开启的时候才有效果。
	# minimum word prefix length to index
	# optional, default is 0 (do not index prefixes)
	# 最小前缀索引长度
    # 为什么要有这个配置项呢？
    # 首先这个是当启用通配符配置启用的前提下说的，前缀索引使得一个关键词产生了多个索引项，导致索引文件体积和搜索时间增加巨大。
    # 那么我们就有必要限制下前缀索引的前缀长度，比如example，当前缀索引长度设置为5的时候，它只会分解为exampl，example了。
	# min_prefix_len		= 0


	# minimum word infix length to index
	# optional, default is 0 (do not index infixes)
	# 最小索引中缀长度。理解同上
	# min_infix_len		= 0 # 此值最小值为2


	# maximum substring (prefix or infix) length to index
	# optional, default is 0 (do not limit substring length)
	#
	# max_substring_len	= 8


	# list of fields to limit prefix/infix indexing to
	# optional, default value is empty (index all fields in prefix/infix mode)
	# 前缀索引和中缀索引字段列表。并不是所有的字段都需要进行前缀和中缀索引。
	# prefix_fields		= filename
	# infix_fields		= url, domain


	# expand keywords with exact forms and/or stars when searching fit indexes
	# search-time only, does not affect indexing, can be 0 or 1
	# optional, default is 0 (do not expand keywords)
	# 词汇展开
    # 是否尽可能展开关键字的精确格式或者型号形式
	# expand_keywords		= 1

	
	# n-gram length to index, for CJK indexing 对于非字母型数据的长度切割(for CJK indexing) 
	# only supports 0 and 1 for now, other lengths to be implemented
	# optional, default is 0 (disable n-grams)
	# 简单分词，搜索中文必须设置为1（只有0和1两个选项）
	ngram_len		= 1 # 已验证，对于搜索中文来讲，此配置必须打开

	# n-gram characters list, for CJK indexing 需要分词的字符
	# optional, default is empty
	# N-Gram索引的分词技术 
    # N-Gram是指不按照词典，而是按照字长来分词，这个主要是针对非英文体系的一些语言来做的（中文、韩文、日文）
	ngram_chars		= U+3000..U+2FA1F # 已验证，对于搜索中文来讲，此配置必须打开


	# phrase boundary characters list
	# optional, default is empty
	# 词组边界符列表
	# 哪些字符被看做分隔不同词组的边界
	# phrase_boundary		= ., ?, !, U+2026 # horizontal ellipsis


	# phrase boundary word position increment
	# optional, default is 0
	# 词组边界符步长
	# phrase_boundary_step	= 100


	# blended characters list
	# blended chars are indexed both as separators and valid characters
	# for instance, AT&T will results in 3 tokens ("at", "t", and "at&t")
	# optional, default is empty
	# 混合字符列表
	# blend_chars		= +, &, U+23


	# blended token indexing mode
	# a comma separated list of blended token indexing variants
	# known variants are trim_none, trim_head, trim_tail, trim_both, skip_pure
	# optional, default is trim_none
	# 混合字符模式
	# blend_mode		= trim_tail, skip_pure


	# whether to strip HTML tags from incoming documents
	# known values are 0 (do not strip) and 1 (do strip)
	# optional, default is 0
	# html标记清理，是否从输出全文数据中去除HTML标记。默认0：不清除； 1：清除html标记
	html_strip		= 1

	# what HTML attributes to index if stripping HTML
	# optional, default is empty (do not index anything)
	# HTML标记属性索引设置。
	# html_index_attrs	= img=alt,title; a=title;


	# what HTML elements contents to strip
	# optional, default is empty (do not strip element contents)
	# 需要清理的html元素
	html_remove_elements	= style, script


	# whether to preopen index data files on startup
	# optional, default is 0 (do not preopen), searchd-only
	# searchd是预先打开全部索引还是每次查询再打开索引。默认0：不打开，仅搜索时打开索引； 1：每次启动searchd时会打开全部索引
	preopen			= 1
	
	# 字典文件是保持在磁盘上还是将他预先缓冲在内存中。（貌似在最新版中此配置已废除）
    # ondisk_dict       = 1


	# whether to enable in-place inversion (2x less disk, 90-95% speed)
	# optional, default is 0 (use separate temporary files), indexer-only
	# 由于在索引建立的时候，需要建立临时文件和和副本，还有旧的索引
    # 这个时候磁盘使用量会暴增，于是有个方法是临时文件重复利用
    # 这个配置会极大减少建立索引时候的磁盘压力，代价是索引建立速度变慢。
	# inplace_enable		= 1

	# in-place fine-tuning options
	# optional, defaults are listed below
	#
	# inplace_hit_gap		= 0 # preallocated hitlist gap size
	# inplace_docinfo_gap	= 0 # preallocated docinfo gap size
	# inplace_reloc_factor	= 0.1 # relocation buffer size within arena
	# inplace_write_factor	= 0.1 # write buffer size within arena


	# whether to index original keywords along with stemmed versions
	# enables "=exactform" operator to work
	# optional, default is 0
	# 词形处理后是否还要检索原词
	# index_exact_words	= 1


	# position increment on overshort (less that min_word_len) words
	# optional, allowed values are 0 and 1, default is 1
	# 在经过过短的位置后增加位置值
	# overshort_step		= 1


	# position increment on stopword
	# optional, allowed values are 0 and 1, default is 1
	# 在经过 停用词 处后增加位置值
	# stopword_step		= 1


	# hitless words list
	# positions for these keywords will not be stored in the index
	# optional, allowed values are 'all', or a list file name
	# 位置忽略词汇列表
	# hitless_words		= all
	# hitless_words		= hitless.txt


	# detect and index sentence and paragraph boundaries
	# required for the SENTENCE and PARAGRAPH operators to work
	# optional, allowed values are 0 and 1, default is 0
	# 是否检测并索引句子和段落边界
	# index_sp			= 1


	# index zones, delimited by HTML/XML tags
	# a comma separated list of tags and wildcards
	# required for the ZONE operator to work
	# optional, default is empty string (do not index zones)
	# 字段内需要索引的HTML/XML区域的标签列表
	# index_zones		= title, h*, th


	# index per-document and average per-index field lengths, in tokens
	# required for the BM25A(), BM25F() in expression ranker
	# optional, default is 0 (do not index field lenghts)
	#
	# index_field_lengths	= 1


	# regular expressions (regexps) to filter the fields and queries with
	# gets applied to data source fields when indexing
	# gets applied to search queries when searching
	# multi-value, optional, default is empty list of regexps
	#
	# regexp_filter		= \b(\d+)\" => \1inch
	# regexp_filter		= (blue|red) => color


	# list of the words considered frequent with respect to bigram indexing
	# optional, default is empty
	#
	# bigram_freq_words	= the, a, i, you, my


	# bigram indexing mode
	# known values are none, all, first_freq, both_freq
	# option, default is none (do not index bigrams)
	#
	# bigram_index		= both_freq


	# snippet document file name prefix
	# preprended to file names when generating snippets using load_files option
	# WARNING, this is a prefix (not a path), trailing slash matters!
	# optional, default is empty
	#
	# snippets_file_prefix	= /mnt/mydocs/server1


	# whether to apply stopwords before or after stemming
	# optional, default is 0 (apply stopwords after stemming)
	#
	# stopwords_unstemmed	= 0


	# path to a global (cluster-wide) keyword IDFs file
	# optional, default is empty (use local IDFs)
	#
	# global_idf		= /usr/local/sphinx/var/global.idf
}


# inherited index example
#
# all the parameters are copied from the parent index,
# and may then be overridden in this index definition
index test1stemmed : test1 # 索引也是可以继承的
{
	path			= /var/data/test1stemmed
	morphology		= stem_en
}


# distributed index example
#
# this is a virtual index which can NOT be directly indexed,
# and only contains references to other local and/or remote indexes
index dist1
{
	# 'distributed' index type MUST be specified
	type			= distributed # 分布式索引

	# local index to be searched
	# there can be many local indexes configured
	local			= test1
	local			= test1stemmed

	# remote agent
	# multiple remote agents may be specified
	# syntax for TCP connections is 'hostname:port:index1,[index2[,...]]'
	# syntax for local UNIX connections is '/path/to/socket:index1,[index2[,...]]'
	# 分布式索引（distributed index）中的远程代理和索引声明
	agent			= localhost:9313:remote1
	agent			= localhost:9314:remote2,remote3
	# agent			= /var/run/searchd.sock:remote4

	# remote agent mirrors groups, aka mirrors, aka HA agents
	# defines 2 or more interchangeable mirrors for a given index part
	#
	# agent			= server3:9312 | server4:9312 :indexchunk2
	# agent			= server3:9312:chunk2server3 | server4:9312:chunk2server4
	# agent			= server3:chunk2server3 | server4:chunk2server4
	# agent			= server21|server22|server23:chunk2


	# blackhole remote agent, for debugging/testing
	# network errors and search results will be ignored
	# 分布式索引（ distributed index）中声明远程黑洞代理
	# agent_blackhole		= testbox:9312:testindex1,testindex2


	# persistenly connected remote agent
	# reduces connect() pressure, requires that workers IS threads
	#
	# agent_persistent		= testbox:9312:testindex1,testindex2


	# remote agent connection timeout, milliseconds
	# optional, default is 1000 ms, ie. 1 sec
	# 远程代理的连接超时时间
	agent_connect_timeout	= 1000

	# remote agent query timeout, milliseconds
	# optional, default is 3000 ms, ie. 3 sec
	# 远程查询超时时间
	agent_query_timeout		= 3000

	# HA mirror agent strategy
	# optional, defaults to ??? (random mirror)
	# know values are nodeads, noerrors, roundrobin, nodeadstm, noerrorstm
	#
	# ha_strategy				= nodeads
}


# realtime index example
#
# you can run INSERT, REPLACE, and DELETE on this index on the fly
# using MySQL protocol (see 'listen' directive below)
index rt
{
	# 'rt' index type must be specified to use RT index
	type			= rt # 实时索引

	# index files path and file name, without extension
	# mandatory, path must be writable, extensions will be auto-appended
	path			= /var/data/rt

	# RAM chunk size limit
	# RT index will keep at most this much data in RAM, then flush to disk
	# optional, default is 128M
	# RT索引内存限制
	# rt_mem_limit		= 512M

	# full-text field declaration
	# multi-value, mandatory
	# 全文字段定义
	rt_field		= title
	rt_field		= content

	# unsigned integer attribute declaration
	# multi-value (an arbitrary number of attributes is allowed), optional
	# declares an unsigned 32-bit attribute
	# 无符号整数属性定义
	rt_attr_uint		= gid

	# RT indexes currently support the following attribute types:
	# uint, bigint, float, timestamp, string, mva, mva64, json
	# 各种属性定义。属性的定义见上面的sql_attr_xxx
	# rt_attr_bigint		= guid
	# rt_attr_float		= gpa
	# rt_attr_timestamp	= ts_added
	# rt_attr_string		= author
	# rt_attr_multi		= tags
	# rt_attr_multi_64	= tags64
	# rt_attr_json		= extra_data
}

#############################################################################
## indexer settings
#############################################################################

indexer # 索引总设置
{
	# memory limit, in bytes, kiloytes (16384K) or megabytes (256M)
	# optional, default is 128M, max is 2047M, recommended is 256M to 1024M
	# 建立索引的时候，索引内存限制
	mem_limit		= 128M

	# maximum IO calls per second (for I/O throttling)
	# optional, default is 0 (unlimited)
	# 每秒最大I/O操作次数，用于限制I/O操作
	# max_iops		= 40


	# maximum IO call size, bytes (for I/O throttling)
	# optional, default is 0 (unlimited)
	# 最大允许的I/O操作大小，以字节为单位，用于I/O节流。默认0：不限制
	# max_iosize		= 1048576


	# maximum xmlpipe2 field length, bytes
	# optional, default is 2M
	# 对于XMLLpipe2数据源允许的最大的字段大小，以字节为单位
	# max_xmlpipe2_field	= 4M


	# write buffer size, bytes
	# several (currently up to 4) buffers will be allocated
	# write buffers are allocated in addition to mem_limit
	# optional, default is 1M
	# 写缓冲区的大小，单位是字节
	# write_buffer		= 1M


	# maximum file field adaptive buffer size
	# optional, default is 8M, minimum is 1M
	# 文件字段可用的最大缓冲区大小，字节为单位
	# max_file_field_buffer	= 32M


	# how to handle IO errors in file fields
	# known values are 'ignore_field', 'skip_document', and 'fail_index'
	# optional, default is 'ignore_field'
	#
	# on_file_field_error = skip_document


	# lemmatizer cache size
	# improves the indexing time when the lemmatization is enabled
	# optional, default is 256K
	#
	# lemmatizer_cache = 512M
}

#############################################################################
## searchd settings
#############################################################################

searchd # 搜索服务配置
{
	# [hostname:]port[:protocol], or /unix/socket/path to listen on
	# known protocols are 'sphinx' (SphinxAPI) and 'mysql41' (SphinxQL)
	#
	# multi-value, multiple listen points are allowed
	# optional, defaults are 9312:sphinx and 9306:mysql41, as below
	# 监听ip和端口
	# listen			= 127.0.0.1
	# listen			= 192.168.0.1:9312
	# listen			= 9312
	# listen			= /var/run/searchd.sock
	listen			= 9312
	listen			= 9306:mysql41

	# log file, searchd run info is logged here
	# optional, default is 'searchd.log'
	log			= /var/log/searchd.log # 监听日志保存路径

	# query log file, all search queries are logged here
	# optional, default is empty (do not log queries)
	query_log		= /var/log/query.log # 查询日志保存路径

	# client read timeout, seconds
	# optional, default is 5
	read_timeout		= 5  # 客户端读超时时间；单位：秒

	# request timeout, seconds
	# optional, default is 5 minutes
	client_timeout		= 300 # 客户端持久连接超时时间，即客户端读一次以后，持久连接，然后再读一次。中间这个持久连接的时间。

	# maximum amount of children to fork (concurrent searches to run)
	# optional, default is 0 (unlimited)
	max_children		= 30 # 并行执行搜索的数目

	# maximum amount of persistent connections from this master to each agent host
	# optional, but necessary if you use agent_persistent. It is reasonable to set the value
	# as max_children, or less on the agent's hosts.
	persistent_connections_limit	= 30

    # 守护进程在内存中为每个索引所保持并返回给客户端的匹配数目的最大值（此参数估计在最新版中已废除）
    # max_matches     = 1000
    
	# PID file, searchd process ID file name
	# mandatory
	pid_file		= /var/log/searchd.pid  # 进程id文件；如： pid_file = /data/logs/sphinx/searchd.pid

	# seamless rotate, prevents rotate stalls if precaching huge datasets
	# optional, default is 1
	# 无缝轮转。防止 searchd 轮换在需要预取大量数据的索引时停止响应
    # 当进行索引轮换的时候，可能需要消耗大量的时间在轮换索引上。
    # 但是启动了无缝轮转，就以消耗内存为代价减少轮转的时间
    # 是否支持无缝切换(做增量索引时需要)
	seamless_rotate		= 1

	# whether to forcibly preopen all indexes on startup
	# optional, default is 1 (preopen everything)
	preopen_indexes		= 1 # 索引预开启，是否强制重新打开所有索引文件

	# whether to unlink .old index copies on succesful rotation.
	# optional, default is 1 (do unlink)
	unlink_old		= 1 # 索引轮换成功之后，是否删除以.old为扩展名的索引拷贝（是否释放旧的索引文件）

	# attribute updates periodic flush timeout, seconds
	# updates will be automatically dumped to disk this frequently
	# optional, default is 0 (disable periodic flush)
	# 属性刷新周期
    # 就是使用UpdateAttributes()更新的文档属性每隔多少时间写回到磁盘中。单位秒
	# attr_flush_period	= 900


	# MVA updates pool size MVA更新池大小(默认为1M) 
	# shared between all instances of searchd, disables attr flushes!
	# optional, default size is 1M
	mva_updates_pool	= 1M # 用于多值属性MVA更新的存储空间的内存共享池大小

	# max allowed network packet size
	# limits both query packets from clients, and responses from agents
	# optional, default size is 8M
	max_packet_size		= 8M #  网络通讯时允许的最大的包的大小

	# max allowed per-query filter count
	# optional, default is 256
	max_filters		= 256 # 每次查询允许设置的过滤器的最大个数

	# max allowed per-filter values count
	# optional, default is 4096
	max_filter_values	= 4096 # 单个过滤器允许的值的最大个数


	# socket listen queue length
	# optional, default is 5
	# TCP监听待处理队列长度
	# listen_backlog		= 5


	# per-keyword read buffer size
	# optional, default is 256K
	# 每个关键字的读缓冲区的大小
	# read_buffer		= 256K


	# unhinted read size (currently used when reading hits)
	# optional, default is 32K
	# 无匹配时读操作的大小
	# read_unhinted		= 32K


	# max allowed per-batch query count (aka multi-query count)
	# optional, default is 32
	max_batch_queries	= 32 # 每次批量查询的查询数限制


	# max common subtree document cache size, per-query
	# optional, default is 0 (disable subtree optimization)
	# 每个查询的公共子树文档缓存大小
	# subtree_docs_cache	= 4M


	# max common subtree hit cache size, per-query
	# optional, default is 0 (disable subtree optimization)
	# 每个查询的公共子树命中缓存大小
	# subtree_hits_cache	= 8M


	# multi-processing mode (MPM)
	# known values are none, fork, prefork, and threads
	# threads is required for RT backend to work
	# optional, default is threads
	# 多处理模式（MPM）。 可选项；可用值为none、fork、prefork，以及threads。 默认在Unix类系统为form，Windows系统为threads。
	workers			= threads # for RT to work


	# max threads to create for searching local parts of a distributed index
	# optional, default is 0, which means disable multi-threaded searching
	# should work with all MPMs (ie. does NOT require workers=threads)
	# 并发查询线程数
	# dist_threads		= 4


	# binlog files path; use empty string to disable binlog
	# optional, default is build-time configured data directory
	# 二进制日志路径
	# binlog_path		= # disable logging
	binlog_path = /data/logs/sphinx/data # binlog.001 etc will be created there


	# binlog flush/sync mode
	# 0 means flush and sync every second
	# 1 means flush and sync every transaction
	# 2 means flush every transaction, sync every second
	# optional, default is 2
	# 二进制日志刷新模式
	# binlog_flush		= 2


	# binlog per-file size limit
	# optional, default is 128M, 0 means no limit
	# 二进制日志大小限制
	# binlog_max_log_size	= 256M


	# per-thread stack size, only affects workers=threads mode
	# optional, default is 64K
	# 线程堆栈
	# thread_stack			= 128K


	# per-keyword expansion limit (for dict=keywords prefix searches)
	# optional, default is 0 (no limit)
	# 关键字展开限制
	# expansion_limit		= 1000


	# RT RAM chunks flush period
	# optional, default is 0 (no periodic flush)
	# RT（增量）索引刷新周期 
	# rt_flush_period		= 900


	# query log file format
	# optional, known values are plain and sphinxql, default is plain
	# 查询日志格式
    # 可选项，可用值为plain、sphinxql，默认为plain。 
	# query_log_format		= sphinxql


	# version string returned to MySQL network protocol clients
	# optional, default is empty (use Sphinx version)
	# MySQL版本设置
	# mysql_version_string	= 5.0.37


	# default server-wide collation
	# optional, default is libc_ci
	# 服务端默认字符集
	# collation_server		= utf8_general_ci


	# server-wide locale for libc based collations
	# optional, default is C
	# 服务端libc字符集
	# collation_libc_locale	= ru_RU.UTF-8


	# threaded server watchdog (only used in workers=threads mode)
	# optional, values are 0 and 1, default is 1 (watchdog on)
	# 线程服务看守
	# watchdog				= 1

	
	# costs for max_predicted_time model, in (imaginary) nanoseconds
	# optional, default is "doc=64, hit=48, skip=2048, match=64"
	#
	# predicted_time_costs	= doc=64, hit=48, skip=2048, match=64


	# current SphinxQL state (uservars etc) serialization path
	# optional, default is none (do not serialize SphinxQL state)
	#
	# sphinxql_state			= sphinxvars.sql


	# maximum RT merge thread IO calls per second, and per-call IO size
	# useful for throttling (the background) OPTIMIZE INDEX impact
	# optional, default is 0 (unlimited)
	#
	# rt_merge_iops			= 40
	# rt_merge_maxiosize		= 1M


	# interval between agent mirror pings, in milliseconds
	# 0 means disable pings
	# optional, default is 1000
	#
	# ha_ping_interval		= 0


	# agent mirror statistics window size, in seconds
	# stats older than the window size (karma) are retired
	# that is, they will not affect master choice of agents in any way
	# optional, default is 60 seconds
	#
	# ha_period_karma			= 60


	# delay between preforked children restarts on rotation, in milliseconds
	# optional, default is 0 (no delay)
	#
	# prefork_rotation_throttle	= 100


	# a prefix to prepend to the local file names when creating snippets
	# with load_files and/or load_files_scatter options
	# optional, default is empty
	#
	# snippets_file_prefix		= /mnt/common/server1/
}

#############################################################################
## common settings
#############################################################################

common # 所有公共配置
{

	# lemmatizer dictionaries base path
	# optional, defaut is /usr/local/share (see ./configure --datadir)
	#
	# lemmatizer_base = /usr/local/share/sphinx/dicts


	# how to handle syntax errors in JSON attributes
	# known values are 'ignore_attr' and 'fail_index'
	# optional, default is 'ignore_attr'
	#
	# on_json_attr_error = fail_index


	# whether to auto-convert numeric values from strings in JSON attributes
	# with auto-conversion, string value with actually numeric data
	# (as in {"key":"12345"}) gets stored as a number, rather than string
	# optional, allowed values are 0 and 1, default is 0 (do not convert)
	#
	# json_autoconv_numbers = 1


	# whether and how to auto-convert key names in JSON attributes
	# known value is 'lowercase'
	# optional, default is unspecified (do nothing)
	#
	# json_autoconv_keynames = lowercase


	# trusted plugin directory
	# optional, default is empty (disable UDFs)
	# 插件目录
	# plugin_dir			= /usr/local/sphinx/lib

}

# --eof--
